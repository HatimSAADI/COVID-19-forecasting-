# -*- coding: utf-8 -*-
"""[Project Analytics Edge] COVID Forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DQUrJELgua77FEyJGBpBAh7VXaVezMxq
"""

pip install pmdarima

"""*Loading important packages*"""

## For data
import pandas as pd
import numpy as np
## For plotting
import matplotlib.pyplot as plt
## For Arima
import pmdarima
import statsmodels.tsa.api as smt
## For Prophet
from fbprophet import Prophet

from datetime import datetime, timedelta

from matplotlib.pylab import rcParams
from pandas import DataFrame
from pylab import rcParams
from sklearn.model_selection import TimeSeriesSplit
from google.colab import drive
import statsmodels.api as sm
import statsmodels.formula.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.diagnostic import normal_ad
from sklearn.model_selection import TimeSeriesSplit
import datetime as dt
import getpass
import graphviz
import hyperopt
import itertools
import seaborn as sns
import sklearn.linear_model as lm
import statsmodels.api as sm
import warnings
from sklearn import preprocessing
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, confusion_matrix, explained_variance_score, mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error
from sklearn.model_selection import train_test_split

pd.options.display.float_format = '{:.2f}'.format   
pd.plotting.register_matplotlib_converters()
rcParams['figure.figsize'] = 15, 5
warnings.filterwarnings('ignore')

"""*Loading the dataset*"""

url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df = pd.read_csv(url)
df = pd.melt(df, id_vars=['Province/State', 'Country/Region','Lat','Long'], var_name='Date', value_name='Confirmed')
df['Date'] = pd.to_datetime(df['Date'])
df.head(5)

"""*Interval of time*"""

df = df.rename(columns={'Country/Region': 'Country'})
df.drop(columns=['Province/State'],inplace=True)
df.head()

"""*Adding a case in country "Worldwide" is about the sum of values of cases of all countries* """

df_worldwide = pd.DataFrame(df.groupby('Date')['Confirmed'].sum())
df_worldwide['Date'] = df_worldwide.index
df_worldwide = df_worldwide.reset_index(drop=True)
df_worldwide['Country'] = 'Worldwide'
df = df.append(df_worldwide)

df_worldwide['Confirmed_lag_1'] = df_worldwide['Confirmed'].shift(1)
df_worldwide['Daily_increase_confirmed'] = df_worldwide['Confirmed']-df_worldwide['Confirmed_lag_1']
df_worldwide.head(5)
df=df_worldwide
df

"""*Grouping dates by daily increase confirmed*"""

ts = df.groupby("Date")["Daily_increase_confirmed"].sum()
ts.head()

"""*Splitting data*"""

def split_train_test(ts, test=0.20, plot=True, figsize=(15,5)):
    ## define splitting point
    if type(test) is float:
        split = int(len(ts)*(1-test))
        perc = test
    elif type(test) is str:
        split = ts.reset_index()[ 
                      ts.reset_index().iloc[:,3]==test].index[0]
        perc = round(len(ts[split:])/len(ts), 2)
    else:
        split = test
        perc = round(len(ts[split:])/len(ts), 2)
    print("--- splitting at index: ", split, "|", 
          ts.index[split], "| test size:", perc, " ---")
    
    ## split ts
    ts_train = ts.head(split)
    ts_test = ts.tail(len(ts)-split)
    if plot is True:
        fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, 
                               sharey=True, figsize=figsize)
        ts_train.plot(ax=ax[0], grid=True, title="Train", 
                      color="black")
        ts_test.plot(ax=ax[1], grid=True, title="Test", 
                     color="black")
        ax[0].set(xlabel=None)
        ax[1].set(xlabel=None)
        plt.show()
        
    return ts_train, ts_test

"""*Visualisation of train and test*"""

#Split_Data
test="2021-01-22"
ts_train, ts_test = split_train_test(ts,test=0.2 , plot=True, figsize=(15,5))
print("train:", len(ts_train), "obs  |  test:", len(ts_test), "obs")

def utils_evaluate_forecast(df, title, plot=True, figsize=(20,13)):
    try:
        ## residuals
        df["residuals"] = df["ts"] - df["model"]
        df["error"] = df["ts"] - df["forecast"]
        df["error_pct"] = df["error"] / df["ts"]
        
        ## kpi
        residuals_mean = df["residuals"].mean()
        residuals_std = df["residuals"].std()
        error_mean = df["error"].mean()
        error_std = df["error"].std()
        mae = df["error"].apply(lambda x: np.abs(x)).mean()
        mape = df["error_pct"].apply(lambda x: np.abs(x)).mean()  
        mse = df["error"].apply(lambda x: x**2).mean()
        rmse = np.sqrt(mse)  #root mean squared error

        
        ## intervals
        df["conf_int_low"] = df["forecast"] - 1.96*residuals_std
        df["conf_int_up"] = df["forecast"] + 1.96*residuals_std
        df["pred_int_low"] = df["forecast"] - 1.96*error_std
        df["pred_int_up"] = df["forecast"] + 1.96*error_std
        
        ## plot
        if plot==True:
            fig = plt.figure(figsize=figsize)
            fig.suptitle(title, fontsize=20)   
            ax1 = fig.add_subplot(2,2, 1)
            ax2 = fig.add_subplot(2,2, 2, sharey=ax1)
            ax3 = fig.add_subplot(2,2, 3)
            ax4 = fig.add_subplot(2,2, 4)
            ### training
            df[pd.notnull(df["model"])][["ts","model"]].plot(color=["black","green"], title="Model", grid=True, ax=ax1)      
            ax1.set(xlabel=None)
            ### test
            df[pd.isnull(df["model"])][["ts","forecast"]].plot(color=["black","red"], title="Forecast", grid=True, ax=ax2)
            ax2.fill_between(x=df.index, y1=df['pred_int_low'], y2=df['pred_int_up'], color='b', alpha=0.2)
            ax2.fill_between(x=df.index, y1=df['conf_int_low'], y2=df['conf_int_up'], color='b', alpha=0.3)     
            ax2.set(xlabel=None)
            ### residuals
            df[["residuals","error"]].plot(ax=ax3, color=["green","red"], title="Residuals", grid=True)
            ax3.set(xlabel=None)
            ### residuals distribution
            df[["residuals","error"]].plot(ax=ax4, color=["green","red"], kind='kde', title="Residuals Distribution", grid=True)
            ax4.set(ylabel=None)
            plt.show()
            print("Training --> Residuals mean:", np.round(residuals_mean), " | std:", np.round(residuals_std))
            print("Test --> Error mean:", np.round(error_mean), " | std:", np.round(error_std),
                  " | mae:",np.round(mae), " | mape:",np.round(mape*100), "%  | mse:",np.round(mse), " | rmse:",np.round(rmse))
        
        return df[["ts","model","residuals","conf_int_low","conf_int_up", 
                    "forecast","error","pred_int_low","pred_int_up"]]
    
    except Exception as e:
        print("--- got error ---")
        print(e)

"""*Building and Finding the best parameters of Arima model* """

best_model = pmdarima.auto_arima(ts,                                    
                                 seasonal=True, stationary=False, 
                                 m=7, information_criterion='aic', 
                                 max_order=20,                                     
                                 max_p=10, max_d=3, max_q=10,                                     
                                 max_P=10, max_D=3, max_Q=10,                                   
                                 error_action='ignore')
print("best model --> (p, d, q):", best_model.order, " and  (P, D, Q, s):", best_model.seasonal_order)

def fit_sarimax(ts_train, ts_test, order=(1,0,1), 
                seasonal_order=(0,0,0,0), exog_train=None, 
                exog_test=None, figsize=(15,10)):
    ## train
    model = smt.SARIMAX(ts_train, order=order, 
                        seasonal_order=seasonal_order, 
                        exog=exog_train, enforce_stationarity=False, 
                        enforce_invertibility=False).fit()
    df_train = ts_train.to_frame(name="ts")
    df_train["model"] = model.fittedvalues
    
    ## test
    df_test = ts_test.to_frame(name="ts")
    df_test["forecast"] = model.predict(start=len(ts_train), 
                            end=len(ts_train)+len(ts_test)-1, 
                            exog=exog_test)
    
    ## evaluate
    df = df_train.append(df_test)
    title = "ARIMA "+str(order) 
    
    title = "S"+title+" x "+str(seasonal_order)
    
    df = utils_evaluate_forecast(df, figsize=figsize, title=title)
    return df, model

df, model = fit_sarimax(ts_train, ts_test, order=(6,1,2), 
                         seasonal_order=(0,0,2,7))

"""*Building the Prophet model*"""

model = Prophet(growth="linear", changepoints=None, 
                n_changepoints=25,
                seasonality_mode="multiplicative",
                yearly_seasonality="auto", 
                weekly_seasonality="auto", 
                daily_seasonality=False,
                holidays=None)

df_train = ts_train.reset_index().rename(columns={"Date":"ds", 
                                                   "Daily_increase_confirmed":"y"})
df_test = ts_test.reset_index().rename(columns={"Date":"ds", 
                                                 "Daily_increase_confirmed":"y"})
df_train.tail()

def fit_prophet(df_train, df_test, lst_exog=None, model=None, 
                freq="D", figsize=(15,10)):
    ## train
    model.fit(df_train)
    
    ## test
    df_prophet = model.make_future_dataframe(periods=len(df_test), 
                  freq=freq, include_history=True)
    df_prophet = model.predict(df_prophet)
  
    df_train = df_train.merge(df_prophet[["ds","yhat"]], 
                how="left").rename(columns={'yhat':'model', 
                'y':'ts'}).set_index("ds")
    df_test = df_test.merge(df_prophet[["ds","yhat"]], 
                how="left").rename(columns={'yhat':'forecast',  
                'y':'ts'}).set_index("ds")

    ## evaluate
    df = df_train.append(df_test)
    df = utils_evaluate_forecast(df, figsize=figsize, 
                                  title="Prophet")
    
    return df, model

df, model = fit_prophet(df_train, df_test, model=model, freq="D")

"""Linear regression"""

df_worldwide

df_worldwide['Day'] = df_worldwide['Date'].dt.day
df_worldwide['Weekday'] = df_worldwide['Date'].dt.weekday
df_worldwide['Week_number'] = df_worldwide['Date'].dt.week
df_worldwide['Quarter'] = df_worldwide['Date'].dt.quarter
df_worldwide['Month'] = df_worldwide['Date'].dt.month
df_worldwide['Year'] = df_worldwide['Date'].dt.year
df_worldwide['Confirmed_lag_1'] = df_worldwide.groupby('Country')['Confirmed'].shift(1)
df_worldwide['Daily_increase_confirmed'] = df_worldwide['Confirmed']-df_worldwide['Confirmed_lag_1']
df_worldwide = df_worldwide.replace([np.inf, -np.inf], np.nan)
df_worldwide = df_worldwide.replace(np.nan, 0)
df_worldwide

df_worldwide = df_worldwide[df_worldwide.columns.difference(['Lat','Long'])]
df_worldwide = df_worldwide.drop_duplicates(subset=['Country','Date'])
df_worldwide['Days_since_outbreak_global'] = df_worldwide.groupby(['Country']).cumcount()+1

df_worldwide['Confirmed_lag_1'] = df_worldwide.groupby('Country')['Confirmed'].shift(1)
df_worldwide['Confirmed'] = df_worldwide['Confirmed']-df_worldwide['Confirmed_lag_1']
df_worldwide['Confirmed_lag_7'] = df_worldwide.groupby('Country')['Confirmed'].shift(7)
df_worldwide['Days_since_outbreak_country'] = df_worldwide.loc[(df_worldwide.Confirmed.notnull())].groupby(['Country']).cumcount()+1
df_worldwide = df_worldwide.replace(np.nan, 0)
df_worldwide = df_worldwide[['Confirmed', 'Country', 'Days_since_outbreak_global', 'Date','Day','Weekday','Week_number','Quarter','Month','Year','Confirmed_lag_7','Days_since_outbreak_country']]

split_date = datetime.today() - timedelta(days=12)
split_date

def mean_absolute_percentage_error(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

#Linear Regression

df_worldwide.index = df_worldwide['Date']

df_worldwide_train= df_worldwide.loc[df_worldwide.Date <= split_date].copy()
df_worldwide_test = df_worldwide.loc[df_worldwide.Date > split_date].copy()

columnsX=['Days_since_outbreak_global','Day','Weekday','Week_number',
          'Quarter','Month','Year', 'Confirmed_lag_7','Days_since_outbreak_country']

X_train, y_train = df_worldwide_train[columnsX], df_worldwide_train['Confirmed']
X_test, y_test = df_worldwide_test[columnsX], df_worldwide_test['Confirmed']

X_train = X_train.fillna(0)
X_test = X_test.fillna(0)

#Create model
linear_regressor = LinearRegression(fit_intercept=False) 
linear_regressor.fit(X_train, y_train)


df_worldwide_test['Confirmed_Prediction'] = linear_regressor.predict(X_test)
Combined = df_worldwide_train.append(df_worldwide_test)
Combined['MA_7_d'] = Combined['Confirmed'].rolling(window=7).mean()

plt.plot(Combined.index, Combined['MA_7_d'], label='MA 7 days', color = 'lightseagreen')
plt.plot(Combined.index, Combined['Confirmed'], label='Train')
plt.plot(df_worldwide_test.index, df_worldwide_test['Confirmed'], label='Test')
plt.plot(Combined.index, Combined['Confirmed_Prediction'], label='Linear Regression')
plt.legend(loc='best')

print('RMSE=',np.sqrt(mean_squared_error(y_pred=df_worldwide_test['Confirmed_Prediction'], y_true=df_worldwide_test['Confirmed'])))
print('MAPE',np.round(mean_absolute_percentage_error(y_true=df_worldwide_test['Confirmed'], y_pred=df_worldwide_test['Confirmed_Prediction'])),'%')

df_worldwide

"""Morocco"""

#Reload Data
url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df = pd.read_csv(url)
df = pd.melt(df, id_vars=['Province/State', 'Country/Region','Lat','Long'], var_name='Date', value_name='Confirmed')
df['Date'] = pd.to_datetime(df['Date'])
df.head()

df = df.rename(columns={'Country/Region': 'Country'})
df.drop(columns=['Province/State'], inplace=True)

df_MAR = df.loc[(df.Country == 'Morocco')]
df_MAR.head()

df_=df.copy()
df_M=df_MAR.copy()

## format datetime column
df["Date"] = pd.to_datetime(df['Date'], format='%d.%m.%Y')
## create time series
ts = df.groupby("Date")["Confirmed"].sum().rename("y")
ts.head

'''
Plot ts with rolling mean and 95% confidence interval with rolling std.
:parameter    
  :param ts: pandas Series    
  :param window: num - for rolling stats
  :param plot_ma: bool - whether plot moving average
  :param plot_intervals: bool - whether plot upper and lower bounds
'''
def plot_ts(ts, plot_ma=True, plot_intervals=True, window=30,
            figsize=(15,5)):    
   rolling_mean = ts.rolling(window=window).mean()    
   rolling_std = ts.rolling(window=window).std()
   plt.figure(figsize=figsize)    
   plt.title(ts.name)    
   plt.plot(ts[window:], label='Actual values', color="black")    
   if plot_ma:        
      plt.plot(rolling_mean, 'g', label='MA'+str(window),
               color="red")    
   if plot_intervals:
      lower_bound = rolling_mean - (1.96 * rolling_std)
      upper_bound = rolling_mean + (1.96 * rolling_std)
   plt.fill_between(x=ts.index, y1=lower_bound, y2=upper_bound,
                    color='lightskyblue', alpha=0.4)
   plt.legend(loc='best')
   plt.grid(True)
   plt.show()

from sklearn import svm

'''
Find outliers using sklearn unsupervised support vetcor machine.
:parameter
    :param ts: pandas Series
    :param perc: float - percentage of outliers to look for
:return
    dtf with raw ts, outlier 1/0 (yes/no), numeric index
'''
def find_outliers(ts, perc=0.01, figsize=(15,5)):
    ## fit svm
    scaler = preprocessing.StandardScaler()
    ts_scaled = scaler.fit_transform(ts.values.reshape(-1,1))
    model = svm.OneClassSVM(nu=perc, kernel="rbf", gamma=0.01)
    model.fit(ts_scaled)
    ## dtf output
    dtf_outliers = ts.to_frame(name="ts")
    dtf_outliers["index"] = range(len(ts))
    dtf_outliers["outlier"] = model.predict(ts_scaled)
    dtf_outliers["outlier"] = dtf_outliers["outlier"].apply(lambda
                                              x: 1 if x==-1 else 0)
    ## plot
    fig, ax = plt.subplots(figsize=figsize)
    ax.set(title="Outliers detection: found"
           +str(sum(dtf_outliers["outlier"]==1)))
    ax.plot(dtf_outliers["index"], dtf_outliers["ts"],
            color="black")
    ax.scatter(x=dtf_outliers[dtf_outliers["outlier"]==1]["index"],
               y=dtf_outliers[dtf_outliers["outlier"]==1]['ts'],
               color='red')
    ax.grid(True)
    plt.show()
    return dtf_outliers

'''
Interpolate outliers in a ts.
'''
def remove_outliers(ts, outliers_idx, figsize=(15,5)):
    ts_clean = ts.copy()
    ts_clean.loc[outliers_idx] = np.nan
    ts_clean = ts_clean.interpolate(method="linear")
    ax = ts.plot(figsize=figsize, color="red", alpha=0.5,
         title="Remove outliers", label="original", legend=True)
    ts_clean.plot(ax=ax, grid=True, color="black",
                  label="interpolated", legend=True)
    plt.show()
    return ts_clean

pip install statsmodels

import statsmodels as sm
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

'''
Test stationarity by:
    - running Augmented Dickey-Fuller test wiht 95%
    - plotting mean and variance of a sample from data
    - plottig autocorrelation and partial autocorrelation
'''

def test_stationarity_acf_pacf(ts, sample=0.20, maxlag=300, figsize= 
                              (15,10)):
    with plt.style.context(style='bmh'):
        ## set figure
        fig = plt.figure(figsize=figsize)
        ts_ax = plt.subplot2grid(shape=(2,2), loc=(0,0), colspan=2)
        pacf_ax = plt.subplot2grid(shape=(2,2), loc=(1,0))
        acf_ax = plt.subplot2grid(shape=(2,2), loc=(1,1))
        
        ## plot ts with mean/std of a sample from the first x% 
        dtf_ts = ts.to_frame(name="ts")
        sample_size = int(len(ts)*sample)
        dtf_ts["mean"] = dtf_ts["ts"].head(sample_size).mean()
        dtf_ts["lower"] = dtf_ts["ts"].head(sample_size).mean() + dtf_ts["ts"].head(sample_size).std()
        dtf_ts["upper"] = dtf_ts["ts"].head(sample_size).mean() - dtf_ts["ts"].head(sample_size).std()
        dtf_ts["ts"].plot(ax=ts_ax, color="black", legend=False)
        dtf_ts["mean"].plot(ax=ts_ax, legend=False, color="red",
                            linestyle="--", linewidth=0.7)
        ts_ax.fill_between(x=dtf_ts.index, y1=dtf_ts['lower'], 
                y2=dtf_ts['upper'], color='lightskyblue', alpha=0.4)
        dtf_ts["mean"].head(sample_size).plot(ax=ts_ax,
                legend=False, color="red", linewidth=0.9)
        ts_ax.fill_between(x=dtf_ts.head(sample_size).index, 
                           y1=dtf_ts['lower'].head(sample_size), 
                           y2=dtf_ts['upper'].head(sample_size),
                           color='lightskyblue')
        
        ## test stationarity (Augmented Dickey-Fuller)
        adfuller_test = sm.tsa.stattools.adfuller(ts, maxlag=maxlag,
                                                  autolag="AIC")
        adf, p, critical_value = adfuller_test[0], adfuller_test[1], adfuller_test[4]["5%"]
        p = round(p, 3)
        conclusion = "Stationary" if p < 0.05 else "Non-Stationary"
        ts_ax.set_title('Dickey-Fuller Test 95%: '+conclusion+
                        '(p value: '+str(p)+')')
        
        ## pacf (for AR) e acf (for MA) 
        plot_pacf(ts, lags=maxlag, ax=pacf_ax, 
                 title="Partial Autocorrelation (for AR component)")
        plot_acf(ts, lags=maxlag, ax=acf_ax,
                 title="Autocorrelation (for MA component)")
        plt.tight_layout()

df_MAR['Confirmed_lag_1'] = df_MAR['Confirmed'].shift(1)
df_MAR['Daily_increase_confirmed'] = df_MAR['Confirmed']-df_MAR['Confirmed_lag_1']

df_MAR.drop(columns=['Confirmed_lag_1','Confirmed','Lat','Long','Country'],inplace=True)
df_MAR = df_MAR.drop(187)
df_MAR = df_MAR.reset_index(drop=True)
df_MAR.tail()

df_MAR=df_MAR.drop(0)

## format datetime column
df_MAR["Date"] = pd.to_datetime(df_MAR['Date'], format='%d.%m.%Y')
## create time series
ts = df_MAR.groupby("Date")["Daily_increase_confirmed"].sum().rename("y")
ts.head

ts.plot(figsize=(15,10),color='#C00000')

plot_ts(ts, window=7, figsize=(10,7))

df_outliers = find_outliers(ts, perc=0.05)

test_stationarity_acf_pacf(ts, sample=0.20, maxlag=30)

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts, freq=7)
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid   
fig, ax = plt.subplots(nrows=4, ncols=1, sharex=True, sharey=False)
ax[0].plot(ts)
ax[0].set_title('Original')
ax[0].grid(True) 
ax[1].plot(trend)
ax[1].set_title('Trend')
ax[1].grid(True)  
ax[2].plot(seasonal)
ax[2].set_title('Seasonality')
ax[2].grid(True)  
ax[3].plot(residual)
ax[3].set_title('Residuals')
ax[3].grid(True)

pip install pmdarima

import pmdarima
best_model = pmdarima.auto_arima(ts,                                    
                                 seasonal=True, stationary=False, 
                                 m=7, information_criterion='aic', 
                                 max_order=20,                                     
                                 max_p=10, max_d=3, max_q=10,                                     
                                 max_P=10, max_D=3, max_Q=10,                                   
                                 error_action='ignore')
print("best model --> (p, d, q):", best_model.order, " and  (P, D, Q, s):", best_model.seasonal_order)

import statsmodels.api as sm
model=sm.tsa.statespace.SARIMAX(df_MAR['Daily_increase_confirmed'],order=(5,1,0),seasonal_order=(1,0,1,7))
results=model.fit()
df_MAR['forecast']=results.predict(start=342,end=372,dynamic=False)
plt.plot(df_MAR['Date'],df_MAR[['Daily_increase_confirmed','forecast']])

L=[]
for i in range(1,342):
  L.append(i)
y_actual = df_MAR['Daily_increase_confirmed'].drop(L)
y_predicted = df_MAR['forecast'].dropna().to_numpy()

RMSE = mean_squared_error(y_actual, y_predicted, squared=False)
MAPE = mean_absolute_percentage_error(y_actual, y_predicted)
print('RMSE=', RMSE, 'MAPE=', MAPE)

import statsmodels.api as sm
model=sm.tsa.statespace.SARIMAX(df_MAR['Daily_increase_confirmed'],order=(6, 2, 2),seasonal_order=(2,0,0,7))
results=model.fit()
df_MAR['forecast']=results.predict(start=342,end=372,dynamic=False)
plt.plot(df_MAR['Date'],df_MAR[['Daily_increase_confirmed','forecast']])

df_MAR = df.loc[(df.Country == 'Morocco')]
df_MAR.head()

df_MAR['Day'] = df_MAR['Date'].dt.day
df_MAR['Weekday'] = df_MAR['Date'].dt.weekday
df_MAR['Week_number'] = df_MAR['Date'].dt.week
df_MAR['Quarter'] = df_MAR['Date'].dt.quarter
df_MAR['Month'] = df_MAR['Date'].dt.month
df_MAR['Year'] = df_MAR['Date'].dt.year
df_MAR['Confirmed_lag_1'] = df_MAR.groupby('Country')['Confirmed'].shift(1)
df_MAR['Daily_increase_confirmed'] = df_MAR['Confirmed']-df_MAR['Confirmed_lag_1']
df_MAR = df_MAR.replace([np.inf, -np.inf], np.nan)
df_MAR = df_MAR.replace(np.nan, 0)
df_MAR

df_MAR = df_MAR[df_MAR.columns.difference(['Lat','Long'])]
df_MAR = df_MAR.drop_duplicates(subset=['Country','Date'])
df_MAR['Days_since_outbreak_global'] = df_MAR.groupby(['Country']).cumcount()+1

df_MAR['Confirmed_lag_1'] = df_MAR.groupby('Country')['Confirmed'].shift(1)
df_MAR['Confirmed'] = df_MAR['Confirmed']-df_MAR['Confirmed_lag_1']
df_MAR['Confirmed_lag_7'] = df_MAR.groupby('Country')['Confirmed'].shift(7)
df_MAR['Days_since_outbreak_country'] = df_MAR.loc[(df_MAR.Confirmed.notnull())].groupby(['Country']).cumcount()+1
df_MAR = df_MAR.replace(np.nan, 0)
df_MAR = df_MAR[['Confirmed', 'Country', 'Days_since_outbreak_global', 'Date','Day','Weekday','Week_number','Quarter','Month','Year','Confirmed_lag_7','Days_since_outbreak_country']]

split_date = datetime.today() - timedelta(days=15)
split_date

#Linear Regression

df_MAR.index = df_MAR['Date']

df_MAR_train= df_MAR.loc[df_MAR.Date <= split_date].copy()
df_MAR_test = df_MAR.loc[df_MAR.Date > split_date].copy()

columnsX=['Days_since_outbreak_global','Day','Weekday','Week_number',
          'Quarter','Month','Year', 'Confirmed_lag_7','Days_since_outbreak_country']

X_train, y_train = df_MAR_train[columnsX], df_MAR_train['Confirmed']
X_test, y_test = df_MAR_test[columnsX], df_MAR_test['Confirmed']

X_train = X_train.fillna(0)
X_test = X_test.fillna(0)

#Create model
linear_regressor = LinearRegression(fit_intercept=False) 
linear_regressor.fit(X_train, y_train)


df_MAR_test['Confirmed_Prediction'] = linear_regressor.predict(X_test)
Combined = df_MAR_train.append(df_MAR_test)
Combined['MA_7_d'] = Combined['Confirmed'].rolling(window=7).mean()

plt.plot(Combined.index, Combined['MA_7_d'], label='MA 7 days', color = 'lightseagreen')
plt.plot(Combined.index, Combined['Confirmed'], label='Train')
plt.plot(df_MAR_test.index, df_MAR_test['Confirmed'], label='Test')
plt.plot(Combined.index, Combined['Confirmed_Prediction'], label='Linear Regression')
plt.legend(loc='best')

print(np.sqrt(mean_squared_error(y_pred=df_MAR_test['Confirmed_Prediction'], y_true=df_MAR_test['Confirmed'])))
print('MAPE',np.round(mean_absolute_percentage_error(y_true=df_worldwide_test['Confirmed'], y_pred=df_worldwide_test['Confirmed_Prediction'])),'%')

model = Prophet(growth="linear", changepoints=None, 
                n_changepoints=25,
                seasonality_mode="multiplicative",
                yearly_seasonality="auto", 
                weekly_seasonality="auto", 
                daily_seasonality=False,
                holidays=None)

df_train = ts_train.reset_index().rename(columns={"Date":"ds", 
                                                   "Daily_increase_confirmed":"y"})
df_test = ts_test.reset_index().rename(columns={"Date":"ds", 
                                                 "Daily_increase_confirmed":"y"})
df_train.tail()

def fit_prophet(df_train, df_test, lst_exog=None, model=None, 
                freq="D", figsize=(15,10)):
    ## train
    model.fit(df_train)
    
    ## test
    df_prophet = model.make_future_dataframe(periods=len(df_test), 
                  freq=freq, include_history=True)
    df_prophet = model.predict(df_prophet)
  
    df_train = df_train.merge(df_prophet[["ds","yhat"]], 
                how="left").rename(columns={'yhat':'model', 
                'y':'ts'}).set_index("ds")
    df_test = df_test.merge(df_prophet[["ds","yhat"]], 
                how="left").rename(columns={'yhat':'forecast',  
                'y':'ts'}).set_index("ds")

    ## evaluate
    df = df_train.append(df_test)
    df = utils_evaluate_forecast(df, figsize=figsize, 
                                  title="Prophet")
    
    return df, model

df, model = fit_prophet(df_train, df_test, model=model, freq="D")

dfre

#get back the initial data
df=df_
df_MAR=df_M

#prepare for arima
df_MAR['Confirmed_lag_1'] = df_MAR['Confirmed'].shift(1)
df_MAR['Daily_increase_confirmed'] = df_MAR['Confirmed']-df_MAR['Confirmed_lag_1']
df_MAR.drop(columns=['Confirmed_lag_1','Confirmed','Lat','Long','Country'],inplace=True)
df_MAR = df_MAR.drop(187)
df_MAR = df_MAR.reset_index(drop=True)
df_MAR.tail()

# Train the model on the full dataset 
from statsmodels.tsa.statespace.sarimax import SARIMAX 
model = model = SARIMAX(df_MAR['Daily_increase_confirmed'],  
                        order = (6, 1, 1),seasonal_order =(1, 0, 0, 7))
result = model.fit() 
  
# Forecast for the next 3 years 
forecast = result.predict(start = len(df_MAR),  
                          end = (len(df_MAR)) + 3 * 7,  
                          typ = 'levels').rename('Forecast') 
  
# Plot the forecast values 
df_MAR['Daily_increase_confirmed'].plot(figsize = (15, 7), legend = True) 
forecast.plot(legend = True)